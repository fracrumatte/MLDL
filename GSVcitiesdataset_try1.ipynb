{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1921bHkYfFbh6vZRbdqZSJmKFP_XW_4MI",
      "authorship_tag": "ABX9TyPR1J6i2pXBuZDOd+wTX7LL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fracrumatte/MLDL/blob/main/GSVcitiesdataset_try1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrYAPQtemMN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGBDKy6FmHNN"
      },
      "outputs": [],
      "source": [
        "# https://github.com/amaralibey/gsv-cities\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "import  re\n",
        "\n",
        "default_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# NOTE: Hard coded path to dataset folder\n",
        "BASE_PATH = '/content/drive/MyDrive/geoloc_fcm/extracted_datasets/gsv_xs'\n",
        "\n",
        "if not Path(BASE_PATH).exists():\n",
        "    raise FileNotFoundError(\n",
        "        'BASE_PATH is hardcoded, please adjust to point to gsv_cities')\n",
        "\n",
        "class GSVCitiesDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 cities=['London', 'Boston'],\n",
        "                 img_per_place=4,\n",
        "                 min_img_per_place=4,\n",
        "                 random_sample_from_each_place=True,\n",
        "                 transform=default_transform,\n",
        "                 base_path=BASE_PATH\n",
        "                 ):\n",
        "        super(GSVCitiesDataset, self).__init__()\n",
        "        self.base_path = base_path\n",
        "        self.cities = cities\n",
        "\n",
        "        assert img_per_place <= min_img_per_place, \\\n",
        "            f\"img_per_place should be less than {min_img_per_place}\"\n",
        "        self.img_per_place = img_per_place\n",
        "        self.min_img_per_place = min_img_per_place\n",
        "        self.random_sample_from_each_place = random_sample_from_each_place\n",
        "        self.transform = transform\n",
        "\n",
        "        # generate the dataframe contraining images metadata\n",
        "        self.dataframe = self.__getdataframes()\n",
        "\n",
        "        # get all unique place ids\n",
        "        self.places_ids = pd.unique(self.dataframe.index)\n",
        "        self.total_nb_images = len(self.dataframe)\n",
        "\n",
        "    def __getdataframes(self):\n",
        "        '''\n",
        "            Return one dataframe containing\n",
        "            all info about the images from all cities\n",
        "\n",
        "            This requieres DataFrame files to be in a folder\n",
        "            named Dataframes, containing a DataFrame\n",
        "            for each city in self.cities\n",
        "        '''\n",
        "        # read the first city dataframe\n",
        "        df = pd.read_csv(self.base_path+'Dataframes/'+f'{self.cities[0]}.csv') # ---NB remember to modify this string, the path doesnt have Dataframes/---\n",
        "        df = df.sample(frac=1)  # shuffle the city dataframe\n",
        "\n",
        "\n",
        "        # append other cities one by one\n",
        "        for i in range(1, len(self.cities)):\n",
        "            tmp_df = pd.read_csv(\n",
        "                self.base_path+'Dataframes/'+f'{self.cities[i]}.csv')\n",
        "\n",
        "            # Now we add a prefix to place_id, so that we\n",
        "            # don't confuse, say, place number 13 of NewYork\n",
        "            # with place number 13 of London ==> (0000013 and 0500013)\n",
        "            # We suppose that there is no city with more than\n",
        "            # 99999 images and there won't be more than 99 cities\n",
        "            # TODO: rename the dataset and hardcode these prefixes\n",
        "            prefix = i\n",
        "            tmp_df['place_id'] = tmp_df['place_id'] + (prefix * 10**5)\n",
        "            tmp_df = tmp_df.sample(frac=1)  # shuffle the city dataframe\n",
        "\n",
        "            df = pd.concat([df, tmp_df], ignore_index=True)\n",
        "\n",
        "        # keep only places depicted by at least min_img_per_place images\n",
        "        res = df[df.groupby('place_id')['place_id'].transform(\n",
        "            'size') >= self.min_img_per_place]\n",
        "        return res.set_index('place_id')\n",
        "\n",
        "    def __getdffromimgs__(path):\n",
        "      data=[]\n",
        "      image_dir=path\n",
        "\n",
        "      for filename in os.listdir(image_dir):\n",
        "        if filename.endswith('.jpg'):  # Add more extensions if needed\n",
        "          with Image.open(\"hopper.jpg\") as im:\n",
        "            image = im\n",
        "          text=filename\n",
        "          pattern = r'@([^@]+)@'\n",
        "          text = text.replace('@','@@')\n",
        "          text = text.replace('_','@@')\n",
        "          matches = re.findall(pattern, text)\n",
        "          data.append(matches.append(im))\n",
        "      return pd.DataFrame(data, columns=['x1','x2','x3','x4','lat','lon'])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        place_id = self.places_ids[index]\n",
        "\n",
        "        # get the place in form of a dataframe (each row corresponds to one image)\n",
        "        place = self.dataframe.loc[place_id]\n",
        "\n",
        "        # sample K images (rows) from this place\n",
        "        # we can either sort and take the most recent k images\n",
        "        # or randomly sample them\n",
        "        if self.random_sample_from_each_place:\n",
        "            place = place.sample(n=self.img_per_place)\n",
        "        else:  # always get the same most recent images\n",
        "            place = place.sort_values(\n",
        "                by=['year', 'month', 'lat'], ascending=False)\n",
        "            place = place[: self.img_per_place]\n",
        "\n",
        "        imgs = []\n",
        "        for i, row in place.iterrows():\n",
        "            img_name = self.get_img_name(row)\n",
        "            img_path = self.base_path + 'Images/' + \\\n",
        "                row['city_id'] + '/' + img_name\n",
        "            img = self.image_loader(img_path)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            imgs.append(img)\n",
        "\n",
        "        # NOTE: contrary to image classification where __getitem__ returns only one image\n",
        "        # in GSVCities, we return a place, which is a Tesor of K images (K=self.img_per_place)\n",
        "        # this will return a Tensor of shape [K, channels, height, width]. This needs to be taken into account\n",
        "        # in the Dataloader (which will yield batches of shape [BS, K, channels, height, width])\n",
        "        return torch.stack(imgs), torch.tensor(place_id).repeat(self.img_per_place)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''Denotes the total number of places (not images)'''\n",
        "        return len(self.places_ids)\n",
        "\n",
        "    @staticmethod\n",
        "    def image_loader(path):\n",
        "        return Image.open(path).convert('RGB')\n",
        "\n",
        "    @staticmethod\n",
        "    def get_img_name(row):\n",
        "        # given a row from the dataframe\n",
        "        # return the corresponding image name\n",
        "\n",
        "        city = row['city_id']\n",
        "\n",
        "        # now remove the two digit we added to the id\n",
        "        # they are superficially added to make ids different\n",
        "        # for different cities\n",
        "        pl_id = row.name % 10**5  #row.name is the index of the row, not to be confused with image name\n",
        "        pl_id = str(pl_id).zfill(7)\n",
        "\n",
        "        panoid = row['panoid']\n",
        "        year = str(row['year']).zfill(4)\n",
        "        month = str(row['month']).zfill(2)\n",
        "        northdeg = str(row['northdeg']).zfill(3)\n",
        "        lat, lon = str(row['lat']), str(row['lon'])\n",
        "        name = city+'_'+pl_id+'_'+year+'_'+month+'_' + \\\n",
        "            northdeg+'_'+lat+'_'+lon+'_'+panoid+'.jpg'\n",
        "        return name\n"
      ]
    }
  ]
}